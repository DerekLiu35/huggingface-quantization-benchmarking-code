defaults:
  - benchmark
  - scenario: inference
  - launcher: process
  - backend: pytorch
  - _base_
  - _self_

name: llama_gptq_marlin

launcher:
  device_isolation: true
  device_isolation_action: warn

backend:
  quantization_scheme: gptq
  quantization_config:
      bits: 4
      backend: marlin
      dataset: "c4"  # Calibration dataset; can use your own data
      group_size: 128
  model: hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4

scenario:
  memory: true
  latency: true
  input_shapes:
    batch_size: 1
    sequence_length: 64
  generate_kwargs:
    max_new_tokens: 64
    min_new_tokens: 64
    do_sample: false
    num_beams: 1